---
layout : posts
author : Mingu Lee
title : "[민구/2nd meetup] Extended ML 키워드 조사"
published : true
---

1. 머신러닝 이론
- 회귀와 분류의 공통점과 차이점
회귀와 분류는 지도학습(Supervised)의 종류인데 분류(Classification)이란 주어진 데이터를 정해진 카테고리에 따라 분류하는 방법이다.
예시로는 스팸분류가 있는데 이메일은 스팸메일이거나 정상적인 메일로 라벨링이 되어 있어서
구분 할 수 있고 이처럼 맞다 / 아니다로 구분되는 문제를 해결할 때 분류를 확인하지만 모두 맞다 / 아니다로 구분되지는 않으며
여러 가지 분류로 나눌 때에는 Multi-label Classification을 사용한다
다음으로 회귀(Regression)은 연속된 값을 예측할 때에 사용하는 방법인데 어떤 패턴이나 트렌드, 경향을 예측할 때 사용한다 
예시로는 공부시간에 따른 시험 점수를 예측할 때 사용하기도 한다.

-선형 회귀란?
선형 회귀(Linear Regression)란 집의 평수가 클수록, 집의 매매 가격이 비싼 ‘경향’이 있듯이
어떤 요인의 수치에 따라서 특정 요인의 수치가 영향을 받을 때, 어떤 변수의 값에 따라서
특정 변수의 값이 영향을 받을 때 나오는 문제를 해결할 때 사용하는 방법이다.
이때의 독립변수와 종속병수의 선형 관계를 모델링하는 분석기법인데 독립변수는 1개가 아니라 그 이상일수 있으며
이에 따라 더 나뉘어서 단순 선형 회귀분석과 다중 선형 회귀분석으로 나눌 수 있다.
단순 회귀 분석은 수식으로 y=Ax+b의 형태를 나타나며 여기서 독립변수에 곱해지는 값을 가중치, 별도로 더해지는 값을 편향이라고 하는데
직선의 방정식에서는 각각 기울기와 절편을 의미한다.
다중 선형 회귀분석은 수식으로 y=Ax+Bx+Cx+...+b의 형태를 나타내며 종속변수에 영향을 주는 다른 독립변수들도 있다고 가정을 한 것이다.

- 손실에 대하여 (L2손실, L1손실 등)
모델을 학습할 때 데이터로부터 올바른 가중치와 편향값을 학습해야하는데
이에 따른 손실은 잘못된 예측에 대한 차이값이며 가설에서 모델의 예측이 얼마나 잘못되었는지를 나타내는 수이다.
가설이 완벽하면 당연히 손실값은 0이며 그렇지 않으면 손실값은 이보다 커질 것이다.
가장 널리 쓰이는 손실함수는 평균 제곱 오차인데 가설과 실제의 차이를 모두 제곱한 다음에 평균을 낸 것이 MSE이다.
MSE는 데이터가 예측으로부터 얼마나 퍼져있는지는 잘 나타내며 제곱을 하는 이유는
평균이 0이되지않기위해 모든 값을 제곱해서 부호를 ‘+’로 바꿔주는 것이다.
제곱대신에 절대값을 사용하는 방법도 있는데 이를 평균 절대 오차(MAE)라고 한다.
MSB보다 계산하기가 더 쉽고 MSE는 오차를 제곱하기 때문에 이상점(Outlier)에 영향을 적게받으며 이를 통계용어로 강건(robust)하다고 한다.
MSE와 MAE를 절충한 후버 손실(Huber loss)라는 것도 있는데 일정한 범위를 정해서 그 안에 있으면 오차를 제곱하고, 그 밖은 오차의 절대값을 구한 것이다.

- L2 와 L1의 정규화. 그리고 이 둘의 비교
정규화(Regularization)은 회귀계수들에 제약을 가해 일반화 성능을 높이는 기법으로 모델의 분산을 감소시켜서 성능을을 높이는 기법이다.
선형회귀 모델은 종속변수의 실제값과 예측값 사이의 MSE를 최소화하는 회귀계소들의 집합을 가리키는데
이러한 회귀계수를 뽑는데 쓰는 기법을 최소자승법(LSM)이라고 한다.
이를 통해 구한 값은 편향이 없는 예측값 가운데 분산이 제일 작다고 하여서 BLUE(Best Linear Unbiased Estimator)라고 한다.
이에는 릿지회귀, 라쏘회귀, 엘라스틱넷이 있다.
![image01.png](C:\asm)


- 손실을 줄이는 방법 - 경사하강법과 학습률에 대하여
머신러닝은 결국 손실을 줄이기 위한 가중치와 편향을 찾기위한 작업을 하는 것인데
이때 사용되는 것이 최적화 알고리즘인 옵티마이저(Optimizer)이며 이 옵티마이저의 가장 기본적인 알고리즘의 경사 하강법은
MSE를 구해서 최적의 가중치값을 찾아가기 위한 비용함수를 가중치에 대해서
편미분해준 것에 학습률인 파라미터를 곱한 것을 초기 설정된 가중치값에서 빼준다
이 과정을 반복하면 최적의 가중치 값을 찾을 수 있고
비용함수와 가중치의 그래프에서의 그래프의 접선의 경사가 점차 감소하기 때문에 경사감소법이라고 불린다.

- 과적합이란?
과적합(overfitting)은 머신러닝에서 모델을 만들 때 학습데이터를 과하게 학습시키는 것을 말한다.
이는 일반화 성능을 떨어뜨리는 요소중에 하나이며 학습데이터에 좋은 성능을 내고 오차나 MSE가 줄어들 수 있지만
새로운 데이터에는 적용이 되지 않을 수 있다.
내가 세운 모델이 과적합인지 판별을 하려면 새로운 데이터를 넣어보거나 모델의 복잡도를 보거나
설명 변수가 너무 많아서 차원의 저주(The curse of dimensionality)에 빠진지 확인하는 것이다.

- 특성 벡터, 특성 추출이란
특성 벡터(Feature Vector)란 데이터를 레이블로 분류할 때
특성 추출과정을 거쳐서 피처의 개수를 명확히 규정시켜놓고 데이터를 N차원의 벡터로 표현한 것이다.
이러한 특성 추출을 학습한다면 선형을 분류할때에 필요한 특성을 편향 없이 추출할 수 있고
사람이 부과하는 제약에 의존하지 않고 필요한 데이터를 추출 할 수 있어서 높은 성능을 보여줄 수 있다.

- OOV(Out Of Vocabulary)
OOV(Out Of Vocabulary)은 자연어 처리(NLP)에서 빈번히 발생하는 데이터 문제로
입력 언어(input language)가 db or input of embedding에 없어서 처리를 못하는 문제로
이를 해결하기 위해서 문자 임베딩(character embedding)이 사용되기도 한다.

- 원 핫 인코딩, 멀티 핫 인코딩
원 핫 인코딩(One-hot encoding)은 자연어처리에서 문자를 숫자로 바꾸는 여러기법들중 가장 기본적인 표현방법이다.
단어 집합의 크기를 벡터의 차원으로 하고 표현하고 싶은 단어의 인덱스에 1(Hot,True)의 값을 부여하고 
다른 인덱스에는 0(Cold,False)을 부여하는 단어의 벡터 표현 방식이다.
하지만 이러한 표현 방식은 단어의 개수가 늘어날수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있다.
또한 단어의 유사도를 표현하지 못해서 검색 시스템에서 심각한 문제를 일으킨다는 단점도 존재한다.

- 시그모이드 함수, ReLU 함수의 특징과 적용
시그모이드(Sigmoid) 함수란 S자의 완만한 커브 형태를 보이는 함수로 1/1+e^-x로 나타낼 수 있다.
이는 모든 실수 입력 값을 0~1사이의 미분 가능한 수로 변환하는 특징을 가지고 있고
로지스틱 분류 문제의 가설이나 비용함수에 많이 사용이 되며 시그모이드 함수의 반환 값은 확률 형태이기 때문에 결과를 확률로 해석할 때 많이 사용한다.
또한 이러한 특징 때문에 초기에는 많이 사용이 되었다가
마이너스 값을 0에 가깝게 표현하여 입력값이 최종 계층에서 미치는 영향이 적어지는 Vanishing gradiendt가 발생하기에
현재 딥러닝에는 사용하지 않는다.
이러한 문제를 해결하기 위한 함수가 ReLU(Rectified Linear Unit)함수이다.
이는 0보다 작을때는 0을 사용하고 0보다 큰 값에 대해서는 해당 값을 그대로 사용하여 이러한 문제를 줄였다.

- 로지스틱 회귀란? 로지스틱 회귀 모델에서의 정규화, 손실, 임계값
로지스틱 회귀는(Logistic Regression)은 분류하려는 데이터가 2가지 범주로 나눠진 경우에 적용되며
일반적으로는 이항 로지스틱을 사용하고 이보다 더 많을 경우에는 다항 로지스틱을 사용한다.
로지스틱 회귀분석은 주어진 데이터를 대표하는 하나의 직선(회귀선)을 찾는 선형 회귀분석에서부터 비롯되는데
우리가 예측하려는 확률을 A,B로 나뉘는데 x값이 (-∞,∞)인데 확률이 0~1의 값만 갖기에 (-∞,∞)으로 바꿔주기 위해 탄생한 것이다.
이를 위해 선형 회귀식 y=ax+b를 최소제곱법으로 회귀계수 a와 b를 구하고
로그오즈(오즈[odds]란 어떤 일이 일어날 승산이며 P/1-P로 정의되고 P는 특정 사건이 일어날 확률)에 관한 선형 관계식으로 정리해서 그래프를 그리면
시그모이드 함수가 나온다.
이처럼 로지스틱 회귀에서는 입력 함수의 반환값에 대해 가중치 업데이트 여부를 결정하는 활성 함수로 시그모이드 함수를 사용한다.
이에 정규화를 적용하여 회귀 분석을 수행하는 방법에는
릿지 회귀(Ridge Regression), 라쏘(Least Absolute Shrinkage and Selection Operator), Elastic Net이 있다.
로지스틱 회귀에서의 임계값은 로지스틱 회귀가 확률을 반환할 때 회귀 값을 이진 카테고리에 매핑해서 분류할 기준이다.

2. NN (Neural Network)
- 신경망이란?
생물학의 신경망은 중추신경계중 특히 뇌에 있는 것으로 인공 신경망은 인간의 뇌를 프로그래밍하고자 하는 시도에서부터 탄생했다
뇌에는 전기적인 신호를 전달하는 신경세포인 뉴런있는데 뉴런에서 수상 돌기(input)에서 신호를 받아들이고 축색 돌기(output)에서 신호를 전송하며
일정 기준(임계값)이상의 전기 신호가 존재해야 신호가 전달이 된다.
이를 통해 뉴런의 시스템 모델링을 할 수 있는데 입력값과 함수, 출력값을 통해 표현할 수 있지만
실제 뇌에 뉴런은 약 1천억개 가량 존재하고 100조개의 연결이 존재한다.
그러기에 엄청난 계산량으로 인해 한계점을 찾았지만 이는 컴퓨터가 1초에 조~경 단위로 계산을 실행하는 성능이기에 해결했고
실제로 알파고는 13개의 input과 output사이의 층으로 수백~수책만개의 연결 고리를 갖고 있었다.

- NN의 구조 (어떻게 구성되는가?)
뉴런의 시스템 모델링은 입력값, 함수, 출력값을 통해 표현이 가능한데 수학적으로 표현하면 F(x)=Wx로 표현할 수 있다.
이는 입력값(x)에 가중치(W)를 두어 값(F(x))를 구하고 그 값과 임계치와의 관계를 활성함수로 판단하여 결과값을 출력하는 모델이라는 것을 확인시켜준다.
이때 활성함수는 뉴런에서 임계값을 넘었을 떄만 출력하는 부분을 표현하는 방법으로는 step함수, sigmoid 함수, Relu함수등 많은 방법이 있다.
이런 활성 함수들의 성능도 각기 달라서 어떤 함수를 쓰느냐에 따라서 학습 속도나 정확도가 달라지기도 한다.

- 전향 전파법 (forward propagation)
전향 전파법은 input 훈련 데이터로부터 출력값을 계산하고
각 출력 뉴런에서의 오차를 계산하며 입력값에서 층을 통해 출력값으로 정보가 흘러가기 때문에 forward propagation으로 불린다.
- 시그모이드 함수를 어디에 적용하는가?
![image02.png](https://miro.medium.com/max/2998/1*0TGxsf-xrp2ELhNfBWNfHA.png)
![image03.png](https://miro.medium.com/max/598/0*2rD1AiR62dfq7J3O.)    

- 역전파법 (Backpropagation)
역전파법은 네트워크가 끝나는 부분에서 시작되어 네트워크가 추측하는 것이 얼마나 잘못됐는지 보이는 것이며
입력값에 도달하기까지 가중치를 조정하면서 반대방향으로 진행된다.
![image04.png](https://d262ilb51hltx0.cloudfront.net/max/1600/1*cywgo_I0fAPw4QqGh8gwRg.png) 
